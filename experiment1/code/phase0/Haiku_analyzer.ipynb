{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c60e4c20-64a2-45fd-8c44-7f02aece3355",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python3\n",
    "\"\"\"\n",
    "Simple Topic Analyzer - Just Works!\n",
    "\"\"\"\n",
    "\n",
    "import json\n",
    "import os\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import Counter\n",
    "import pandas as pd\n",
    "from matplotlib.backends.backend_pdf import PdfPages\n",
    "import ipywidgets as widgets\n",
    "from IPython.display import display, clear_output\n",
    "from anthropic import Anthropic\n",
    "\n",
    "class SimpleTopicAnalyzer:\n",
    "    def __init__(self):\n",
    "        self.api_key = \"\"\n",
    "        self.client = None\n",
    "        self.results = {}\n",
    "        self.full_analysis = []  # Store all individual analyses\n",
    "        self.all_keywords = []   # Store all keywords for topic identification\n",
    "        \n",
    "        # Interface\n",
    "        self.api_input = widgets.Text(\n",
    "            placeholder='Your Anthropic API key',\n",
    "            description='API Key:',\n",
    "            layout=widgets.Layout(width='400px')\n",
    "        )\n",
    "        \n",
    "        self.file_selector = widgets.Dropdown(\n",
    "            options=self.get_json_files(),\n",
    "            description='JSON File:',\n",
    "            layout=widgets.Layout(width='400px')\n",
    "        )\n",
    "        \n",
    "        self.refresh_btn = widgets.Button(description='Refresh Files', button_style='info')\n",
    "        self.analyze_btn = widgets.Button(description='Analyze', button_style='success')\n",
    "        \n",
    "        self.output = widgets.Output()\n",
    "        \n",
    "        # Events\n",
    "        self.refresh_btn.on_click(self.refresh_files)\n",
    "        self.analyze_btn.on_click(self.analyze)\n",
    "        \n",
    "        # Layout\n",
    "        interface = widgets.VBox([\n",
    "            widgets.HTML(\"<h2>Simple Topic Analyzer</h2>\"),\n",
    "            self.api_input,\n",
    "            widgets.HBox([self.file_selector, self.refresh_btn]),\n",
    "            self.analyze_btn,\n",
    "            self.output\n",
    "        ])\n",
    "        \n",
    "        display(interface)\n",
    "    \n",
    "    def get_json_files(self):\n",
    "        \"\"\"Get JSON files from the directory.\"\"\"\n",
    "        directory = r\"YOUR_DIRECTORY_HERE\"\n",
    "        try:\n",
    "            files = [f for f in os.listdir(directory) if f.endswith('.json')]\n",
    "            return [(f, os.path.join(directory, f)) for f in files]\n",
    "        except:\n",
    "            return [(\"No files found\", \"\")]\n",
    "    \n",
    "    def refresh_files(self, btn):\n",
    "        \"\"\"Refresh file list.\"\"\"\n",
    "        self.file_selector.options = self.get_json_files()\n",
    "    \n",
    "    def analyze(self, btn):\n",
    "        \"\"\"Run analysis.\"\"\"\n",
    "        self.api_key = self.api_input.value.strip()\n",
    "        file_path = self.file_selector.value\n",
    "        \n",
    "        if not self.api_key:\n",
    "            with self.output:\n",
    "                clear_output()\n",
    "                print(\"‚ùå Enter API key\")\n",
    "            return\n",
    "        \n",
    "        if not file_path or not os.path.exists(file_path):\n",
    "            with self.output:\n",
    "                clear_output()\n",
    "                print(\"‚ùå Select valid file\")\n",
    "            return\n",
    "        \n",
    "        with self.output:\n",
    "            clear_output()\n",
    "            print(\"Starting analysis...\")\n",
    "        \n",
    "        try:\n",
    "            self.client = Anthropic(api_key=self.api_key)\n",
    "            \n",
    "            # Load file\n",
    "            with self.output:\n",
    "                clear_output()\n",
    "                print(\"Loading file...\")\n",
    "            \n",
    "            with open(file_path, 'r', encoding='utf-8') as f:\n",
    "                data = json.load(f)\n",
    "            \n",
    "            # Extract responses\n",
    "            responses = [item['response'] for item in data if 'response' in item]\n",
    "            \n",
    "            with self.output:\n",
    "                clear_output()\n",
    "                print(f\"Found {len(responses)} responses. Extracting keywords...\")\n",
    "            \n",
    "            # Clear previous analysis\n",
    "            self.full_analysis = []\n",
    "            self.all_keywords = []\n",
    "            \n",
    "            # Extract keywords from each response\n",
    "            for i, response in enumerate(responses):\n",
    "                if i % 10 == 0:\n",
    "                    with self.output:\n",
    "                        clear_output()\n",
    "                        print(f\"Extracting keywords {i+1}/{len(responses)}\")\n",
    "                \n",
    "                # Get 5 salient keywords for this response\n",
    "                keywords = self.extract_keywords(response)\n",
    "                \n",
    "                # Store individual analysis\n",
    "                self.full_analysis.append({\n",
    "                    'response_id': i + 1,\n",
    "                    'keywords': ', '.join(keywords),\n",
    "                    'response_preview': response[:100] + '...' if len(response) > 100 else response\n",
    "                })\n",
    "                \n",
    "                # Add to all keywords\n",
    "                self.all_keywords.extend(keywords)\n",
    "                \n",
    "                time.sleep(0.05)\n",
    "            \n",
    "            # Now identify the 10 most relevant topics from all keywords\n",
    "            with self.output:\n",
    "                clear_output()\n",
    "                print(\"Identifying top topics from all keywords...\")\n",
    "            \n",
    "            top_topics = self.identify_top_topics()\n",
    "            \n",
    "            # Count keyword frequency\n",
    "            keyword_counts = Counter(self.all_keywords)\n",
    "            \n",
    "            # Display and save results\n",
    "            file_name = os.path.basename(file_path).replace('.json', '')\n",
    "            self.display_and_save_results(top_topics, keyword_counts, len(responses), file_name)\n",
    "            \n",
    "        except Exception as e:\n",
    "            with self.output:\n",
    "                clear_output()\n",
    "                print(f\"‚ùå Error: {e}\")\n",
    "    \n",
    "    def extract_keywords(self, response):\n",
    "        \"\"\"Extract 5 most salient keywords from a response.\"\"\"\n",
    "        prompt = f\"\"\"Analyze this text and identify the 5 most salient keywords that define the content:\n",
    "\n",
    "\"{response[:500]}...\"\n",
    "\n",
    "Give me exactly 5 keywords that best capture the essence and main concepts of this text.\n",
    "\n",
    "Format: keyword1, keyword2, keyword3, keyword4, keyword5\"\"\"\n",
    "\n",
    "        try:\n",
    "            result = self.client.messages.create(\n",
    "                model=\"claude-3-5-haiku-20241022\",\n",
    "                max_tokens=50,\n",
    "                temperature=0,\n",
    "                messages=[{\"role\": \"user\", \"content\": prompt}]\n",
    "            )\n",
    "            \n",
    "            keywords_text = result.content[0].text.strip()\n",
    "            \n",
    "            # Parse keywords\n",
    "            keywords = [k.strip() for k in keywords_text.split(',')]\n",
    "            # Ensure we have exactly 5, pad with empty if needed\n",
    "            while len(keywords) < 5:\n",
    "                keywords.append(\"\")\n",
    "            \n",
    "            return keywords[:5]\n",
    "            \n",
    "        except:\n",
    "            return [\"\", \"\", \"\", \"\", \"\"]\n",
    "    \n",
    "    def identify_top_topics(self):\n",
    "        \"\"\"Use Haiku to identify 10 most relevant topics from all keywords.\"\"\"\n",
    "        # Get most frequent keywords (top 50) to feed to Haiku\n",
    "        keyword_counts = Counter(self.all_keywords)\n",
    "        top_keywords = [word for word, count in keyword_counts.most_common(50) if word.strip()]\n",
    "        \n",
    "        keywords_text = ', '.join(top_keywords)\n",
    "        \n",
    "        prompt = f\"\"\"Based on these keywords extracted from text responses, identify the 10 most relevant and distinct topics they represent, ordered from MOST FREQUENT to LEAST FREQUENT:\n",
    "\n",
    "Keywords: {keywords_text}\n",
    "\n",
    "Analyze these keywords and group them into 10 distinct, meaningful topics. Consider how often keywords related to each topic appear when determining the order.\n",
    "\n",
    "IMPORTANT: Order your topics from most frequent/common to least frequent/common based on the keyword patterns you see.\n",
    "\n",
    "Format your response as:\n",
    "1. Most Frequent Topic Name\n",
    "2. Second Most Frequent Topic Name\n",
    "3. Third Most Frequent Topic Name\n",
    "(etc. up to 10, ordered by frequency)\n",
    "\n",
    "Make each topic name 2-4 words and distinct from the others.\"\"\"\n",
    "\n",
    "        try:\n",
    "            result = self.client.messages.create(\n",
    "                model=\"claude-3-5-haiku-20241022\",\n",
    "                max_tokens=300,\n",
    "                temperature=0.1,\n",
    "                messages=[{\"role\": \"user\", \"content\": prompt}]\n",
    "            )\n",
    "            \n",
    "            response_text = result.content[0].text.strip()\n",
    "            \n",
    "            # Parse topics\n",
    "            topics = []\n",
    "            for line in response_text.split('\\n'):\n",
    "                line = line.strip()\n",
    "                if line and (line[0].isdigit() or line.startswith('-')):\n",
    "                    topic = line.split('.', 1)[-1].strip()\n",
    "                    if topic:\n",
    "                        topics.append(topic)\n",
    "            \n",
    "            return topics[:10]\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error identifying topics: {e}\")\n",
    "            # Fallback: use most common keywords as topics\n",
    "            return [word for word, count in Counter(self.all_keywords).most_common(10)]\n",
    "    \n",
    "    def display_and_save_results(self, top_topics, keyword_counts, total_responses, file_name):\n",
    "        \"\"\"Display results with plots and save everything.\"\"\"\n",
    "        with self.output:\n",
    "            clear_output()\n",
    "            \n",
    "            # Get top 10 keywords by frequency\n",
    "            top_keywords = keyword_counts.most_common(10)\n",
    "            \n",
    "            # Create output directory\n",
    "            output_dir = r\"YOUR_OUTPUT_DIRECTORY_PATH_HERE\"\n",
    "            os.makedirs(output_dir, exist_ok=True)\n",
    "            \n",
    "            # Create PDF with plots and results\n",
    "            pdf_path = os.path.join(output_dir, f\"{file_name}_analysis.pdf\")\n",
    "            \n",
    "            with PdfPages(pdf_path) as pdf:\n",
    "                # Page 1: Plots\n",
    "                fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 8))\n",
    "                \n",
    "                # Topics plot\n",
    "                if top_topics:\n",
    "                    y_pos = range(len(top_topics))\n",
    "                    # For topics, we'll show them as ranked (no frequency count since they're thematically derived)\n",
    "                    topic_scores = list(range(len(top_topics), 0, -1))  # Reverse ranking as scores\n",
    "                    ax1.barh(y_pos, topic_scores, color='#0d9488')\n",
    "                    ax1.set_yticks(y_pos)\n",
    "                    ax1.set_yticklabels([t[:30] + '...' if len(t) > 30 else t for t in top_topics])\n",
    "                    ax1.set_xlabel('Relevance Rank')\n",
    "                    ax1.set_title(f'Top 10 Topics - {file_name}')\n",
    "                    ax1.invert_yaxis()\n",
    "                \n",
    "                # Keywords plot\n",
    "                if top_keywords:\n",
    "                    keywords, k_counts = zip(*top_keywords)\n",
    "                    y_pos = range(len(keywords))\n",
    "                    ax2.barh(y_pos, k_counts, color='#14b8a6')\n",
    "                    ax2.set_yticks(y_pos)\n",
    "                    ax2.set_yticklabels(keywords)\n",
    "                    ax2.set_xlabel('Frequency')\n",
    "                    ax2.set_title(f'Top 10 Keywords - {file_name}')\n",
    "                    ax2.invert_yaxis()\n",
    "                \n",
    "                plt.tight_layout()\n",
    "                pdf.savefig(fig, bbox_inches='tight')\n",
    "                plt.show()\n",
    "                \n",
    "                # Page 2: Summary Table\n",
    "                fig, ax = plt.subplots(figsize=(12, 8))\n",
    "                ax.axis('tight')\n",
    "                ax.axis('off')\n",
    "                \n",
    "                # Create summary table\n",
    "                summary_data = []\n",
    "                summary_data.append(['ANALYSIS SUMMARY', ''])\n",
    "                summary_data.append(['File', file_name])\n",
    "                summary_data.append(['Total Responses', str(total_responses)])\n",
    "                summary_data.append(['Total Keywords Extracted', str(len(self.all_keywords))])\n",
    "                summary_data.append(['', ''])\n",
    "                summary_data.append(['TOP 10 TOPICS (by relevance)', 'RANK'])\n",
    "                \n",
    "                for i, topic in enumerate(top_topics, 1):\n",
    "                    summary_data.append([f\"{i}. {topic}\", str(i)])\n",
    "                \n",
    "                summary_data.append(['', ''])\n",
    "                summary_data.append(['TOP 10 KEYWORDS (by frequency)', 'COUNT'])\n",
    "                \n",
    "                for i, (keyword, count) in enumerate(top_keywords, 1):\n",
    "                    summary_data.append([f\"{i}. {keyword}\", str(count)])\n",
    "                \n",
    "                # Create table\n",
    "                table = ax.table(cellText=summary_data, cellLoc='left', loc='center')\n",
    "                table.auto_set_font_size(False)\n",
    "                table.set_fontsize(10)\n",
    "                table.scale(1.2, 1.5)\n",
    "                \n",
    "                # Style table\n",
    "                for i in range(len(summary_data)):\n",
    "                    if summary_data[i][0] in ['ANALYSIS SUMMARY', 'TOP 10 TOPICS (by relevance)', 'TOP 10 KEYWORDS (by frequency)']:\n",
    "                        table[(i, 0)].set_facecolor('#0d9488')\n",
    "                        table[(i, 0)].set_text_props(weight='bold', color='white')\n",
    "                        table[(i, 1)].set_facecolor('#0d9488')\n",
    "                        table[(i, 1)].set_text_props(weight='bold', color='white')\n",
    "                \n",
    "                plt.title(f'Analysis Summary - {file_name}', fontsize=16, fontweight='bold', pad=20)\n",
    "                pdf.savefig(fig, bbox_inches='tight')\n",
    "                plt.close()\n",
    "            \n",
    "            # Save full analysis to CSV\n",
    "            csv_path = os.path.join(output_dir, f\"{file_name}_full_analysis.csv\")\n",
    "            df = pd.DataFrame(self.full_analysis)\n",
    "            df.to_csv(csv_path, index=False, encoding='utf-8')\n",
    "            \n",
    "            # Save all keywords to separate CSV\n",
    "            keywords_csv_path = os.path.join(output_dir, f\"{file_name}_all_keywords.csv\")\n",
    "            keywords_df = pd.DataFrame(self.all_keywords, columns=['keyword'])\n",
    "            keywords_df.to_csv(keywords_csv_path, index=False, encoding='utf-8')\n",
    "            \n",
    "            # Save summary to text file\n",
    "            txt_path = os.path.join(output_dir, f\"{file_name}_summary.txt\")\n",
    "            with open(txt_path, 'w', encoding='utf-8') as f:\n",
    "                f.write(f\"TOPIC ANALYSIS RESULTS - {file_name}\\n\")\n",
    "                f.write(\"=\" * 60 + \"\\n\\n\")\n",
    "                f.write(f\"Total responses analyzed: {total_responses}\\n\")\n",
    "                f.write(f\"Total keywords extracted: {len(self.all_keywords)}\\n\\n\")\n",
    "                \n",
    "                f.write(\"TOP 10 TOPICS (by relevance):\\n\")\n",
    "                f.write(\"-\" * 30 + \"\\n\")\n",
    "                for i, topic in enumerate(top_topics, 1):\n",
    "                    f.write(f\"{i}. {topic}\\n\")\n",
    "                \n",
    "                f.write(\"\\nTOP 10 KEYWORDS (by frequency):\\n\")\n",
    "                f.write(\"-\" * 30 + \"\\n\")\n",
    "                for i, (keyword, count) in enumerate(top_keywords, 1):\n",
    "                    f.write(f\"{i}. {keyword}: {count}\\n\")\n",
    "            \n",
    "            # Display results in interface\n",
    "            print(\"TOP 10 TOPICS (identified by Haiku from keywords):\")\n",
    "            print(\"=\" * 60)\n",
    "            for i, topic in enumerate(top_topics, 1):\n",
    "                print(f\"{i}. {topic}\")\n",
    "            \n",
    "            print(\"\\nTOP 10 KEYWORDS (by frequency):\")\n",
    "            print(\"=\" * 50)\n",
    "            for i, (keyword, count) in enumerate(top_keywords, 1):\n",
    "                print(f\"{i}. {keyword}: {count}\")\n",
    "            \n",
    "            print(f\"\\nTotal responses analyzed: {total_responses}\")\n",
    "            print(f\"Total keywords extracted: {len(self.all_keywords)}\")\n",
    "            print(f\"\\nüìÅ FILES SAVED:\")\n",
    "            print(f\"üìä PDF Report: {pdf_path}\")\n",
    "            print(f\"üìã Full Analysis CSV: {csv_path}\")\n",
    "            print(f\"üîë All Keywords CSV: {keywords_csv_path}\")\n",
    "            print(f\"üìÑ Summary TXT: {txt_path}\")\n",
    "            print(\"‚úÖ Done!\")\n",
    "\n",
    "# Create analyzer\n",
    "analyzer = SimpleTopicAnalyzer()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
